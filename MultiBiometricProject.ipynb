{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NameProj\n",
        "The idea of this project is to implement and evaluate a multi-biometric system using Deep Learning.\n"
      ],
      "metadata": {
        "id": "kut0_TeMUC_E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount drive and load data"
      ],
      "metadata": {
        "id": "rRRd3bzSUGd_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "mgos1bofUCph",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59fde9f2-8fc1-46c2-d3db-21517bb4ad37"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "id": "z8ubFsdCzHDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "from google.colab import drive\n",
        "\n",
        "source_folder = '/content/drive/MyDrive/BiometricSystems/SpeakingFaces'\n",
        "destination_folder = '/content/drive/MyDrive/BiometricSystems/SpeakingFacesExt'\n",
        "files_to_delete = []\n",
        "\n",
        "for filename in os.listdir(source_folder):\n",
        "    if files_to_delete != []: print(f\"Please delete these files from {destination_folder}\\n{files_to_delete}\")\n",
        "\n",
        "    if filename.split(\".\")[0] in os.listdir(destination_folder):\n",
        "      print( f\"Unpacked {filename} already exists in the destination folder\")\n",
        "      files_to_delete.append(filename)\n",
        "      continue\n",
        "\n",
        "    if filename.endswith(\".zip\"):\n",
        "        zip_filepath = os.path.join(source_folder, filename)\n",
        "        try:\n",
        "            with zipfile.ZipFile(zip_filepath, 'r') as zip_ref:\n",
        "                # Extract all contents to the destination folder\n",
        "                zip_ref.extractall(destination_folder)\n",
        "            print(f\"Successfully extracted {filename} to {destination_folder}\")\n",
        "\n",
        "            # Delete the unzipped file from the source folder\n",
        "            os.remove(zip_filepath)\n",
        "            print(f\"Deleted {filename} from {source_folder} (Also check manually)\")\n",
        "\n",
        "        except zipfile.BadZipFile:\n",
        "            print(f\"Error: {filename} is not a valid zip file.\")\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred while processing {filename}: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGyVKLQ4nAvI",
        "outputId": "3feaa360-f748-4d83-cd83-142f5ba9b174"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unpacked sub_47_ia.zip already exists in the destination folder\n",
            "Successfully extracted sub_50_ia.zip to /content/drive/MyDrive/BiometricSystems/SpeakingFacesExt\n",
            "Deleted sub_50_ia.zip from /content/drive/MyDrive/BiometricSystems/SpeakingFaces\n",
            "Please delete these files from /content/drive/MyDrive/BiometricSystems/SpeakingFacesExt\n",
            "['sub_47_ia.zip']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Libraries"
      ],
      "metadata": {
        "id": "YmXMkPIDzVPi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "XmxG52SHT4a3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import torchaudio\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm.auto import tqdm\n",
        "import timm\n",
        "import json\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import copy\n",
        "import random\n",
        "import itertools\n",
        "import math\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch.manual_seed(404)\n",
        "np.random.seed(404)\n",
        "random.seed(404)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*The first step is to implement manually a visual transformer that extracts the feature from thermal images*"
      ],
      "metadata": {
        "id": "kwSlxIwBUNMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def weights_init_kaiming(m):\n",
        "    classname = m.__class__.__name__\n",
        "    # print(classname)\n",
        "    if classname.find('Conv') != -1:\n",
        "        init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
        "    elif classname.find('Linear') != -1:\n",
        "        init.kaiming_normal_(m.weight.data, a=0, mode='fan_out')\n",
        "        init.constant_(m.bias.data, 0.0)\n",
        "    elif classname.find('BatchNorm1d') != -1:\n",
        "        init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "def weights_init_classifier(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Linear') != -1:\n",
        "        init.normal_(m.weight.data, std=0.001)\n",
        "        init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "class ClassBlock(nn.Module):\n",
        "    def __init__(self, input_dim, class_num, droprate, relu=False, bnorm=True, num_bottleneck=512, linear=True, return_f = False):\n",
        "        super(ClassBlock, self).__init__()\n",
        "        self.return_f = return_f\n",
        "        add_block = []\n",
        "        if linear:\n",
        "            add_block += [nn.Linear(input_dim, num_bottleneck)]\n",
        "        else:\n",
        "            num_bottleneck = input_dim\n",
        "        if bnorm:\n",
        "            add_block += [nn.BatchNorm1d(num_bottleneck)]\n",
        "        if relu:\n",
        "            add_block += [nn.LeakyReLU(0.1)]\n",
        "        if droprate>0:\n",
        "            add_block += [nn.Dropout(p=droprate)]\n",
        "        add_block = nn.Sequential(*add_block)\n",
        "        add_block.apply(weights_init_kaiming)\n",
        "\n",
        "        classifier = []\n",
        "        classifier += [nn.Linear(num_bottleneck, class_num)]\n",
        "        classifier = nn.Sequential(*classifier)\n",
        "        classifier.apply(weights_init_classifier)\n",
        "\n",
        "        self.add_block = add_block\n",
        "        self.classifier = classifier\n",
        "    def forward(self, x):\n",
        "        x = self.add_block(x)\n",
        "        if self.return_f:\n",
        "            f = x\n",
        "            x = self.classifier(x)\n",
        "            return [x,f]\n",
        "        else:\n",
        "            x = self.classifier(x)\n",
        "            return x\n",
        "\n",
        "class LATransformer(nn.Module):\n",
        "    def __init__(self, model, lmbd, print_verbose = False, test=False, pretraining=False):\n",
        "        super(LATransformer, self).__init__()\n",
        "\n",
        "        if print_verbose:\n",
        "            self._print = print\n",
        "        else:\n",
        "            self._print = lambda *args, **kwargs: None\n",
        "        self.class_num = 751\n",
        "        self.part = 14 # We cut the pool5 to sqrt(N) parts\n",
        "        self.num_blocks = 12\n",
        "        self.model = model\n",
        "        self.model.head.requires_grad_ = False\n",
        "        self.cls_token = self.model.cls_token\n",
        "        self.pos_embed = self.model.pos_embed\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((self.part,768))\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.lmbd = lmbd\n",
        "        self.test = test\n",
        "        self.pretraining = pretraining\n",
        "        if not (self.test or self.pretraining):\n",
        "          for i in range(self.part):\n",
        "              name = 'classifier'+str(i)\n",
        "              setattr(self, name, ClassBlock(768, self.class_num, droprate=0.5, relu=False, bnorm=True, num_bottleneck=256))\n",
        "\n",
        "        if self.pretraining:\n",
        "          self.fc = nn.Sequential(nn.Conv1d(14, 32, 3),\n",
        "                                  nn.BatchNorm1d(32),\n",
        "                                  nn.LeakyReLU(0.1),\n",
        "                                  nn.Conv1d(32, 3, 3),\n",
        "                                  nn.BatchNorm1d(3),\n",
        "                                  nn.LeakyReLU(0.1),\n",
        "                                  nn.Flatten(),\n",
        "                                  nn.Linear(2292, 1024),\n",
        "                                  nn.LeakyReLU(0.1),\n",
        "                                  nn.Linear(1024,128))\n",
        "\n",
        "          self.fc.apply(weights_init_kaiming)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        # Divide input image into patch embeddings and add position embeddings\n",
        "        # cls token is a learnable parameter added to the start of sequence\n",
        "        # It contains global info about the whole image\n",
        "        # Used in classical Transformers like BERT and ViT to do classification with just itself\n",
        "        # Here it is later combined to enrich local features with global features of the image\n",
        "        self._print(f\"x before pos embedding: {x.shape}\")\n",
        "        x = self.model.patch_embed(x)\n",
        "        self._print(f\"x after pos embedding: {x.shape}\")\n",
        "        cls_token = self.cls_token.expand(x.shape[0], -1, -1)\n",
        "        self._print(f\"cls token: {cls_token.shape}\")\n",
        "        x = torch.cat((cls_token, x), dim=1)\n",
        "        self._print(f\"x with concatenation with cls token: {x.shape}\")\n",
        "        x = self.model.pos_drop(x + self.pos_embed)\n",
        "        self._print(f\"x after pos drop: {x.shape}\")\n",
        "\n",
        "        # Feed forward through transformer blocks\n",
        "        for i in range(self.num_blocks):\n",
        "            self._print(f\"x before block {i}: {x.shape}\")\n",
        "            x = self.model.blocks[i](x)\n",
        "        x = self.model.norm(x)\n",
        "        self._print(f\"x after blocks: {x.shape}\")\n",
        "\n",
        "        # extract the cls token\n",
        "        cls_token_out = x[:, 0].unsqueeze(1)\n",
        "        self._print(f\"cls token out: {cls_token_out.shape}\")\n",
        "\n",
        "        # Average pool\n",
        "        x = self.avgpool(x[:, 1:])\n",
        "        self._print(f\"x after avgpool: {x.shape}\")\n",
        "\n",
        "        if self.test:\n",
        "          return x\n",
        "\n",
        "        # Add global cls token to each local token\n",
        "        for i in range(self.part):\n",
        "            self._print(f\"x before mul: {x.shape}\")\n",
        "            out = torch.mul(x[:, i, :], self.lmbd)\n",
        "            x[:,i,:] = torch.div(torch.add(cls_token_out.squeeze(),out), 1+self.lmbd)\n",
        "\n",
        "        if self.pretraining:\n",
        "          x = x.reshape(x.size(0), 14, -1)\n",
        "          x = self.fc(x)\n",
        "          return x\n",
        "\n",
        "        # Locally aware network\n",
        "        part = {}\n",
        "        predict = {}\n",
        "        for i in range(self.part):\n",
        "            part[i] = x[:,i,:]\n",
        "            name = 'classifier'+str(i)\n",
        "            c = getattr(self,name)\n",
        "            predict[i] = c(part[i])\n",
        "        return predict"
      ],
      "metadata": {
        "id": "k0LsWtQgXAJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data üóÉÔ∏è\n"
      ],
      "metadata": {
        "id": "GwfEgxqpu_CI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset has a total of 142 subjects that we can use to finetune each model. The split will be virtually assigned for each model in order to add some noise."
      ],
      "metadata": {
        "id": "X8JuAGdogOvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir= \"./drive/MyDrive/BiometricSystems/SpeakingFacesExt\"\n",
        "\n",
        "train_dir, test_dir , val_dir = \"train\", \"test\", \"val\"\n",
        "subjects_ids = [] #[str(i) for i in range(1,142)]"
      ],
      "metadata": {
        "id": "Ev62T0Abg2eP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking the sanity of the dataset"
      ],
      "metadata": {
        "id": "y1rF0G_ghHpi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for file in os.listdir(data_dir):\n",
        "  if file not in [\"train\", \"val\", \"test\"]:\n",
        "\n",
        "    index = file.split(\"_\")[1]\n",
        "\n",
        "    sub_dir = os.listdir(os.path.join(data_dir, file))\n",
        "    if \"trial_1\" in sub_dir and \"trial_2\" in sub_dir:\n",
        "      subj_id = index\n",
        "      subjects_ids.append(int(subj_id))\n",
        "    #print(file,index)\n",
        "    #shutil.move(os.path.join(data_dir, file), os.path.join(data_dir, train_dir))\n",
        "\n",
        "\n",
        "print(sorted(subjects_ids))\n",
        "print(len(subjects_ids))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yY6ovPK_R-mW",
        "outputId": "27a2fa43-bcb0-4b29-fb5a-5d5871599a53"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 4, 6, 8, 9, 10, 11, 12, 13, 14, 15, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 48, 50, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141]\n",
            "87\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_split(data_path, subjects_ids):\n",
        "  shuffled_data = subjects_ids.copy()\n",
        "  random.shuffle(shuffled_data)\n",
        "  pivot1, pivot2, n = math.ceil(0.7*len(shuffled_data)), math.ceil(0.15*len(shuffled_data)), len(shuffled_data)\n",
        "  print(pivot1)\n",
        "  return shuffled_data[:pivot1], shuffled_data[pivot1:n-pivot2+1], shuffled_data[n-pivot2+1:]"
      ],
      "metadata": {
        "id": "nCtvf8GqhZVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train,val,test = generate_split(data_dir, subjects_ids)\n",
        "print(train)\n",
        "print(val)\n",
        "print(test)\n",
        "print(len(train), len(val), len(test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGj2jZjKi-ym",
        "outputId": "e0c59145-a1ca-4c72-9e87-ada5761c436d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "54\n",
            "[17, 21, 138, 36, 20, 26, 115, 23, 136, 122, 106, 132, 15, 11, 25, 108, 22, 12, 14, 113, 111, 107, 29, 121, 30, 110, 35, 99, 34, 131, 100, 39, 102, 10, 133, 127, 114, 18, 31, 38, 24, 141, 130, 125, 118, 32, 19, 1, 37, 3, 109, 126, 137, 4]\n",
            "[123, 140, 117, 104, 103, 112, 135, 27, 128, 40, 139]\n",
            "[101, 124, 2, 119, 134, 120, 116, 129, 33, 105, 13]\n",
            "54 11 11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Data Loader\n",
        "*We need to define the function to extract the training, validation and test data*"
      ],
      "metadata": {
        "id": "zydKyAZoedcr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, root, labels, transform=None):\n",
        "\n",
        "        self.root = root\n",
        "        self.labels = labels\n",
        "        self.rgb_images_names = {label: [] for label in self.labels}\n",
        "        self.thrml_images_names = {label: [] for label in self.labels}\n",
        "        self.audio_names = {label: [] for label in self.labels}\n",
        "\n",
        "        self.valid_indices = list(self.rgb_images_names.keys())\n",
        "\n",
        "        self.add_data(label_limit=1, data_limit=5)\n",
        "\n",
        "        self.mapping = {int(label): label in self.labels}\n",
        "        self.transform = transform\n",
        "\n",
        "    def data_path_extractor(self, attribute, label, data_limit, subpath, data_folder):\n",
        "      limit_data = data_limit\n",
        "      for data_element in os.listdir(os.path.join(self.root, subpath, data_folder)):\n",
        "          if not limit_data: break\n",
        "          attribute[label] = os.path.join(self.root, subpath, data_folder, data_element)\n",
        "          limit_data -= 1\n",
        "\n",
        "    def add_data(self, label_limit, data_limit):\n",
        "      limit_labels = label_limit\n",
        "      for label in self.labels:\n",
        "          if not limit_labels: break\n",
        "\n",
        "          trial = random.choice([1, 2])\n",
        "          mic_number = random.choice([1, 2])\n",
        "          subpath = os.path.join(f\"sub_{label}_ia\",f\"trial_{trial}\")\n",
        "\n",
        "          self.data_path_extractor(self.rgb_images_names, label, data_limit, subpath, \"rgb_image_cmd_aligned\")\n",
        "          self.data_path_extractor(self.thrml_images_names, label, data_limit, subpath, \"thr_image_cmd\")\n",
        "          self.data_path_extractor(self.audio_names, label, data_limit, subpath, f\"mic{mic_number}_audio_cmd_trim\")\n",
        "          '''\n",
        "          for image_name in os.listdir(os.path.join(self.root, subpath, \"rgb_image_cmd_aligned\")):\n",
        "              if not limit_images: break\n",
        "              self.images_names[label]= os.path.join(self.root, subpath, image_name)\n",
        "              limit_images -= 1\n",
        "          '''\n",
        "          limit_labels -= 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __iter__(self):\n",
        "      for idx in self.valid_indices:\n",
        "        if len(self.rgb_images_names[idx]):\n",
        "            yield self.__getitem__(idx)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.rgb_images_names[idx])\n",
        "        img_thr = Image.open(self.thrml_images_names[idx])\n",
        "        audio_tensor, sample_rate = torchaudio.load(self.audio_names[idx])\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "            img_thr = self.transform(img_thr)\n",
        "        return img, img_thr, (audio_tensor, sample_rate), idx"
      ],
      "metadata": {
        "id": "dE_H9GAvOYOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels, val_labels, test_labels = generate_split(data_dir, subjects_ids)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor() # 256 -> 0.0 ~ 1.0\n",
        "])\n",
        "\n",
        "train_dataset = CustomDataset(data_dir, train_labels, transform=transform)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVmvHgaKPee-",
        "outputId": "05a3d644-b457-4bec-8308-205222bfa0a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "54\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for img, img_thr, audio, label in train_dataset:\n",
        "  a,b,c,d = img, img_thr, audio, label\n",
        "  print(a,b,c,d)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrIb6znN4guf",
        "outputId": "d0683ae0-70a9-4cdc-feb5-0fc0c88368d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.6863, 0.6863, 0.6902,  ..., 0.6824, 0.6824, 0.6824],\n",
            "         [0.6902, 0.6902, 0.6863,  ..., 0.6863, 0.6824, 0.6784],\n",
            "         [0.6902, 0.6902, 0.6902,  ..., 0.6863, 0.6863, 0.6863],\n",
            "         ...,\n",
            "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
            "\n",
            "        [[0.6902, 0.6902, 0.6941,  ..., 0.6902, 0.6902, 0.6902],\n",
            "         [0.6941, 0.6941, 0.6902,  ..., 0.6941, 0.6902, 0.6863],\n",
            "         [0.6941, 0.6941, 0.6941,  ..., 0.6941, 0.6902, 0.6902],\n",
            "         ...,\n",
            "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
            "\n",
            "        [[0.6941, 0.6941, 0.6980,  ..., 0.6824, 0.6784, 0.6784],\n",
            "         [0.6980, 0.6980, 0.6941,  ..., 0.6784, 0.6745, 0.6706],\n",
            "         [0.6980, 0.7020, 0.7020,  ..., 0.6784, 0.6745, 0.6745],\n",
            "         ...,\n",
            "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]]) tensor([[[0.1294, 0.1294, 0.1294,  ..., 0.0000, 0.0000, 0.0000],\n",
            "         [0.1294, 0.1294, 0.1294,  ..., 0.0000, 0.0000, 0.0000],\n",
            "         [0.1294, 0.1294, 0.1294,  ..., 0.0000, 0.0000, 0.0000],\n",
            "         ...,\n",
            "         [0.9373, 0.9490, 0.9608,  ..., 0.0000, 0.0000, 0.0000],\n",
            "         [0.9451, 0.9569, 0.9686,  ..., 0.0000, 0.0000, 0.0000],\n",
            "         [0.9490, 0.9647, 0.9765,  ..., 0.0000, 0.0000, 0.0000]],\n",
            "\n",
            "        [[0.0000, 0.0000, 0.0000,  ..., 0.0471, 0.0471, 0.0510],\n",
            "         [0.0000, 0.0000, 0.0000,  ..., 0.0471, 0.0471, 0.0471],\n",
            "         [0.0000, 0.0000, 0.0000,  ..., 0.0471, 0.0471, 0.0471],\n",
            "         ...,\n",
            "         [0.3216, 0.3451, 0.3725,  ..., 0.0235, 0.0235, 0.0275],\n",
            "         [0.3412, 0.3647, 0.3882,  ..., 0.0235, 0.0275, 0.0275],\n",
            "         [0.3451, 0.3804, 0.4039,  ..., 0.0235, 0.0235, 0.0275]],\n",
            "\n",
            "        [[0.4549, 0.4549, 0.4549,  ..., 0.1882, 0.1843, 0.1765],\n",
            "         [0.4549, 0.4549, 0.4549,  ..., 0.1843, 0.1843, 0.1843],\n",
            "         [0.4549, 0.4549, 0.4549,  ..., 0.1843, 0.1843, 0.1843],\n",
            "         ...,\n",
            "         [0.2471, 0.2196, 0.1922,  ..., 0.2941, 0.2902, 0.2745],\n",
            "         [0.2275, 0.1961, 0.1725,  ..., 0.2863, 0.2784, 0.2706],\n",
            "         [0.2196, 0.1804, 0.1529,  ..., 0.2863, 0.2824, 0.2667]]]) (tensor([[0.0065, 0.0067, 0.0070,  ..., 0.0041, 0.0046, 0.0050]]), 44100) 115\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "def get_data(data_dir=\"/content/BiometricsSystems/data\"):\n",
        "  batch_size = 32\n",
        "\n",
        "  transform_train_list = transforms.Compose([\n",
        "      transforms.Resize((224,224), interpolation=3),\n",
        "      transforms.RandomHorizontalFlip(),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "      ])\n",
        "  transform_val_list = transforms.Compose([\n",
        "      transforms.Resize(size=(224,224),interpolation=3),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "      ])\n",
        "\n",
        "  dataset_train = datasets.ImageFolder(os.path.join(data_dir, 'train'),transform=transform_train_list)\n",
        "  dataset_val = datasets.ImageFolder(os.path.join(data_dir, 'val'),transform=transform_val_list)\n",
        "  train_loader = DataLoader(dataset = dataset_train, batch_size=batch_size, shuffle=True)\n",
        "  val_loader = DataLoader(dataset = dataset_val, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  return train_loader, val_loader\n",
        "  '''"
      ],
      "metadata": {
        "id": "ET1ZhAfSoVXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Freezing Layers ‚ùÑÔ∏è\n",
        "*In order to fine-tune the Visual Transformer we need to keep training the model on new data, without losing the knowledge gained from the pre-trained part. This implies freezing the initial layers and gradually unfreeze the next ones without completely overriding the gained knowledge. This requires lower epochs, dropout and a small learning rate*"
      ],
      "metadata": {
        "id": "hVNjjhITvTt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def freeze_all_blocks(model):\n",
        "    frozen_blocks = 12\n",
        "    for block in model.model.blocks[:frozen_blocks]:\n",
        "        for param in block.parameters():\n",
        "            param.requires_grad=False\n",
        "\n",
        "def unfreeze_blocks(model, amount= 1):\n",
        "    for block in model.model.blocks[11-amount:]:\n",
        "        for param in block.parameters():\n",
        "            param.requires_grad=True\n",
        "    return model\n",
        "\n",
        "def get_training_objects(pretraining_path = None):\n",
        "  backbone = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=751).to(device)\n",
        "  lmbd = 8 #Weight in averaging with CLS token\n",
        "  model = LATransformer(backbone, lmbd).to(device)\n",
        "\n",
        "  if pretraining_path:\n",
        "    model.load_state_dict(torch.load(pretraining_path), strict=False)\n",
        "\n",
        "  freeze_all_blocks(model)\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  optimizer = optim.AdamW(model.parameters(),weight_decay=5e-4, lr=3e-4)\n",
        "\n",
        "  return model, criterion, optimizer"
      ],
      "metadata": {
        "id": "3ZMRB7dCwpx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training ‚è≥‚öôÔ∏è\n",
        "..."
      ],
      "metadata": {
        "id": "8lsL5JjlxauS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(num_epochs, model, train_loader, val_loader, optimizer, loss_fn, save_model_path, log_file):\n",
        "\n",
        "    os.makedirs(save_model_path, exist_ok=True)\n",
        "    log_data = []\n",
        "    unfrozen_blocks = 0\n",
        "    unfreeze_after = 2\n",
        "    lr_decay = .8\n",
        "\n",
        "    with tqdm(total=num_epochs, desc='Total Progress', unit='epoch') as epoch_pbar:\n",
        "      for epoch in range(num_epochs):\n",
        "\n",
        "          if epoch%unfreeze_after==0:\n",
        "            unfrozen_blocks += 1\n",
        "            model = unfreeze_blocks(model, unfrozen_blocks)\n",
        "            optimizer.param_groups[0]['lr'] *= lr_decay\n",
        "            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "            print(\"Unfrozen Blocks: {}, Current lr: {}, Trainable Params: {}\".format(unfrozen_blocks,\n",
        "                                                                                optimizer.param_groups[0]['lr'],\n",
        "                                                                                trainable_params))\n",
        "\n",
        "          model.train()\n",
        "          train_loss = 0.0\n",
        "          train_accuracy = 0.0\n",
        "\n",
        "          with tqdm(train_loader, unit=\"batch\") as pbar:\n",
        "              for data, target in pbar:\n",
        "                  data, target = data.to(device), target.to(device)\n",
        "\n",
        "                  optimizer.zero_grad()\n",
        "                  output = model(data)\n",
        "\n",
        "                  score = sum(nn.Softmax(dim=1)(v) for v in output.values())\n",
        "                  _, preds = torch.max(score, 1)\n",
        "\n",
        "                  loss = sum(loss_fn(v, target) for v in output.values())\n",
        "                  loss.backward()\n",
        "                  optimizer.step()\n",
        "\n",
        "                  train_loss += loss.item()\n",
        "                  train_accuracy += (preds == target).float().mean().item()\n",
        "\n",
        "                  current_loss = train_loss / (pbar.n + 1)\n",
        "                  current_accuracy = train_accuracy / (pbar.n + 1)\n",
        "\n",
        "                  pbar.set_description(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "                  pbar.set_postfix(loss=current_loss, accuracy=current_accuracy)\n",
        "\n",
        "          avg_train_loss = train_loss / len(train_loader)\n",
        "          avg_train_accuracy = train_accuracy / len(train_loader)\n",
        "\n",
        "          model.eval()\n",
        "          val_loss = 0.0\n",
        "          val_accuracy = 0.0\n",
        "\n",
        "          with torch.no_grad():\n",
        "              with tqdm(val_loader, unit=\"batch\") as val_pbar:\n",
        "                  for data, target in val_pbar:\n",
        "                      data, target = data.to(device), target.to(device)\n",
        "\n",
        "                      output = model(data)\n",
        "\n",
        "                      score = sum(nn.Softmax(dim=1)(v) for v in output.values())\n",
        "                      _, preds = torch.max(score, 1)\n",
        "\n",
        "                      loss = sum(loss_fn(v, target) for v in output.values())\n",
        "\n",
        "                      val_loss += loss.item()\n",
        "                      val_accuracy += (preds == target).float().mean().item()\n",
        "\n",
        "                      current_loss = val_loss / (val_pbar.n + 1)\n",
        "                      current_accuracy = val_accuracy / (val_pbar.n + 1)\n",
        "\n",
        "                      val_pbar.set_description(f\"Epoch {epoch+1}/{num_epochs} [Val]\")\n",
        "                      val_pbar.set_postfix(loss=current_loss, accuracy=current_accuracy)\n",
        "\n",
        "          avg_val_loss = val_loss / len(val_loader)\n",
        "          avg_val_accuracy = val_accuracy / len(val_loader)\n",
        "\n",
        "          model_save_path = os.path.join(save_model_path, f'model_epoch_{epoch+1}.pth')\n",
        "          torch.save(model.state_dict(), model_save_path)\n",
        "\n",
        "          epoch_log = {\n",
        "              'epoch': epoch + 1,\n",
        "              'training_loss': avg_train_loss,\n",
        "              'training_accuracy': avg_train_accuracy,\n",
        "              'val_loss': avg_val_loss,\n",
        "              'val_accuracy': avg_val_accuracy\n",
        "          }\n",
        "          log_data.append(epoch_log)\n",
        "\n",
        "          with open(log_file, 'w') as f:\n",
        "              json.dump(log_data, f, indent=4)\n",
        "\n",
        "          print(f\"Epoch {epoch+1}/{num_epochs} - Training Loss: {avg_train_loss:.4f} - Validation Loss: {avg_val_loss:.4f} - Training Accuracy: {avg_train_accuracy:.4f} - Validation Accuracy: {avg_val_accuracy:.4f}\")\n",
        "\n",
        "          epoch_pbar.update(1)\n",
        "          epoch_pbar.set_postfix(\n",
        "                train_loss=avg_train_loss,\n",
        "                train_accuracy=avg_train_accuracy,\n",
        "                val_loss=avg_val_loss,\n",
        "                val_accuracy=avg_val_accuracy\n",
        "          )\n",
        "          print(\"==================================================================================\")\n"
      ],
      "metadata": {
        "id": "sCYPSFQhxbKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(save_path = \"/content/drive/MyDrive/BiometricSystems/LA-Transformers(Vanilla)/\", pretraining_path = None):\n",
        "  train_loader, val_loader = get_data()\n",
        "  model, criterion, optimizer = get_training_objects(pretraining_path)\n",
        "  train(30, model, train_loader, val_loader, optimizer, criterion, save_path, os.path.join(save_path, \"training_results.json\"))"
      ],
      "metadata": {
        "id": "VVdWaeqWxvPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model()"
      ],
      "metadata": {
        "id": "pK28WevMxwFs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}